{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c36b7de2-5221-4b82-850c-047cfb39ecbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] CSV path:      C:\\Users\\sagni\\Downloads\\Price Sense\\archive\\flipkart_com-ecommerce_sample.csv\n",
      "[INFO] Artifact dir:  C:\\Users\\sagni\\Downloads\\Price Sense\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sagni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PriceSense ML Artifacts Ready ===\n",
      "H5   -> C:\\Users\\sagni\\Downloads\\Price Sense\\price_lstm.h5\n",
      "JSON -> C:\\Users\\sagni\\Downloads\\Price Sense\\metrics.json\n",
      "YAML -> C:\\Users\\sagni\\Downloads\\Price Sense\\config.yaml\n",
      "PKL  -> C:\\Users\\sagni\\Downloads\\Price Sense\\fake_review_lr.pkl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# --- Optional: real LSTM if TensorFlow is available; otherwise fallback still writes .h5 ---\n",
    "try:\n",
    "    import tensorflow as tf  # noqa: F401\n",
    "    from tensorflow.keras import Sequential\n",
    "    from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "    TF_AVAILABLE = True\n",
    "except Exception:\n",
    "    TF_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import h5py  # required to write .h5 even in fallback\n",
    "except Exception as e:\n",
    "    raise SystemExit(\"h5py is required. Install with: pip install h5py\") from e\n",
    "\n",
    "\n",
    "# ---------------------- CONFIG (Windows-safe raw strings) ----------------------\n",
    "DATA_CSV = r\"C:\\Users\\sagni\\Downloads\\Price Sense\\archive\\flipkart_com-ecommerce_sample.csv\"\n",
    "ARTIFACT_DIR = r\"C:\\Users\\sagni\\Downloads\\Price Sense\"\n",
    "\n",
    "SETTINGS = {\n",
    "    \"seed_days\": 60,            # synthetic days of price history for demo\n",
    "    \"price_noise_frac\": 0.08,   # Â±8% noise\n",
    "    \"deal_drop_prob\": 0.07,     # 7% chance of extra drop\n",
    "    \"deal_drop_frac\": 0.12,     # 12% extra drop on deal days\n",
    "    \"top_n_products\": 300,      # limit rows for faster first run\n",
    "    \"lstm_sequence_len\": 7,\n",
    "    \"lstm_epochs\": 5,\n",
    "    \"lstm_batch_size\": 64,\n",
    "}\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ---------------------------- Utility functions ------------------------------\n",
    "def ensure_dir(path: str | Path) -> Path:\n",
    "    p = Path(path)\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "    return p\n",
    "\n",
    "\n",
    "def load_csv(csv_path: str | Path) -> pd.DataFrame:\n",
    "    p = Path(csv_path)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"CSV not found at:\\n{p}\\n\"\n",
    "            \"Tip: verify the folder and file name. If the path has spaces, keep it as a raw string (r'...').\"\n",
    "        )\n",
    "    try:\n",
    "        return pd.read_csv(p, encoding=\"utf-8\")\n",
    "    except UnicodeDecodeError:\n",
    "        return pd.read_csv(p, encoding=\"latin1\")\n",
    "\n",
    "\n",
    "def map_columns(df: pd.DataFrame) -> Dict[str, str | None]:\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "\n",
    "    def find(name: str):\n",
    "        for k, v in cols.items():\n",
    "            if name in k:\n",
    "                return v\n",
    "        return None\n",
    "\n",
    "    return {\n",
    "        \"title\": find(\"product_name\") or find(\"title\") or list(df.columns)[0],\n",
    "        \"category\": find(\"product_category_tree\") or find(\"category\"),\n",
    "        \"brand\": find(\"brand\"),\n",
    "        \"retail_price\": find(\"retail_price\") or find(\"mrp\") or find(\"price\"),\n",
    "        \"discounted_price\": find(\"discounted_price\") or find(\"discount_price\"),\n",
    "        \"product_id\": find(\"product_id\") or find(\"pid\"),\n",
    "        \"rating\": find(\"product_rating\") or find(\"rating\"),\n",
    "        \"description\": find(\"description\") or find(\"product_description\"),\n",
    "        \"url\": find(\"product_url\") or find(\"url\"),\n",
    "    }\n",
    "\n",
    "\n",
    "def synthetic_series(base_price: float,\n",
    "                     days: int,\n",
    "                     noise_frac: float,\n",
    "                     deal_prob: float,\n",
    "                     deal_frac: float) -> np.ndarray:\n",
    "    prices = []\n",
    "    p = float(max(1.0, base_price))\n",
    "    for _ in range(days):\n",
    "        noise = np.random.uniform(-noise_frac, noise_frac) * p\n",
    "        p = max(1.0, p + noise)\n",
    "        if random.random() < deal_prob:\n",
    "            p = max(1.0, p * (1.0 - deal_frac))\n",
    "        prices.append(p)\n",
    "    return np.array(prices, dtype=np.float32)\n",
    "\n",
    "\n",
    "def make_sequences(series: np.ndarray, seq_len: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    X, y = [], []\n",
    "    for i in range(len(series) - seq_len):\n",
    "        X.append(series[i:i + seq_len])\n",
    "        y.append(series[i + seq_len])\n",
    "    return np.array(X, dtype=np.float32), np.array(y, dtype=np.float32)\n",
    "\n",
    "\n",
    "def train_lstm_and_save(all_series: List[np.ndarray],\n",
    "                        seq_len: int,\n",
    "                        epochs: int,\n",
    "                        batch_size: int,\n",
    "                        h5_out: Path) -> Dict:\n",
    "    \"\"\"\n",
    "    Trains a tiny LSTM if TensorFlow is available; otherwise writes a valid .h5 placeholder.\n",
    "    \"\"\"\n",
    "    metrics: Dict = {\"trained\": False, \"framework\": \"tensorflow\" if TF_AVAILABLE else \"fallback\"}\n",
    "\n",
    "    # Build dataset (normalize each series by its max)\n",
    "    Xs, ys = [], []\n",
    "    for s in all_series:\n",
    "        s = np.asarray(s, dtype=np.float32)\n",
    "        if len(s) <= seq_len + 1:\n",
    "            continue\n",
    "        scale = max(1e-6, float(s.max()))\n",
    "        s_norm = s / scale\n",
    "        X, y = make_sequences(s_norm, seq_len)\n",
    "        Xs.append(X[..., None])   # add feature dim (T, 1)\n",
    "        ys.append(y)\n",
    "\n",
    "    if not Xs:\n",
    "        ensure_dir(h5_out.parent)\n",
    "        with h5py.File(h5_out, \"w\") as f:\n",
    "            f.create_dataset(\"note\", data=np.string_(\"No data to train.\"))\n",
    "        metrics.update({\"trained\": False, \"reason\": \"no_data\"})\n",
    "        return metrics\n",
    "\n",
    "    X_all = np.vstack(Xs)\n",
    "    y_all = np.hstack(ys)\n",
    "\n",
    "    if TF_AVAILABLE:\n",
    "        try:\n",
    "            model = Sequential([\n",
    "                LSTM(32, input_shape=(X_all.shape[1], 1), return_sequences=False),\n",
    "                Dropout(0.2),\n",
    "                Dense(16, activation=\"relu\"),\n",
    "                Dense(1),\n",
    "            ])\n",
    "            model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "            hist = model.fit(\n",
    "                X_all, y_all,\n",
    "                epochs=int(epochs),\n",
    "                batch_size=int(batch_size),\n",
    "                verbose=0,\n",
    "                validation_split=0.2,\n",
    "                shuffle=True,\n",
    "            )\n",
    "            ensure_dir(h5_out.parent)\n",
    "            model.save(h5_out)\n",
    "            metrics.update({\n",
    "                \"trained\": True,\n",
    "                \"epochs\": int(epochs),\n",
    "                \"final_loss\": float(hist.history[\"loss\"][-1]),\n",
    "                \"final_val_loss\": float(hist.history.get(\"val_loss\", [0])[-1]),\n",
    "            })\n",
    "            return metrics\n",
    "        except Exception as e:\n",
    "            metrics.update({\"trained\": False, \"error\": f\"TF failed: {e}\"})\n",
    "\n",
    "    # Fallback: write minimal HDF5 note\n",
    "    ensure_dir(h5_out.parent)\n",
    "    with h5py.File(h5_out, \"w\") as f:\n",
    "        f.create_dataset(\"note\", data=np.string_(\"TensorFlow unavailable; using placeholder model.\"))\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# -------------------- Fake Review (heuristic) model --------------------\n",
    "@dataclass\n",
    "class ReviewFeatures:\n",
    "    length: int\n",
    "    exclam: int\n",
    "    caps_ratio: float\n",
    "    unique_ratio: float\n",
    "    digits: int\n",
    "    words: int\n",
    "\n",
    "\n",
    "def featurize(text: str) -> ReviewFeatures:\n",
    "    import re\n",
    "    t = text or \"\"\n",
    "    words = re.findall(r\"[A-Za-z0-9']+\", t)\n",
    "    words_lower = [w.lower() for w in words]\n",
    "    unique_ratio = (len(set(words_lower)) / (len(words_lower) + 1e-6))\n",
    "    caps_ratio = (sum(1 for c in t if c.isupper()) / (len(t) + 1e-6))\n",
    "    digits = sum(ch.isdigit() for ch in t)\n",
    "    return ReviewFeatures(\n",
    "        length=len(t),\n",
    "        exclam=t.count(\"!\"),\n",
    "        caps_ratio=float(caps_ratio),\n",
    "        unique_ratio=float(unique_ratio),\n",
    "        digits=int(digits),\n",
    "        words=len(words),\n",
    "    )\n",
    "\n",
    "\n",
    "def weak_label(text: str) -> int:\n",
    "    \"\"\"Heuristic: 1 = suspicious, 0 = genuine.\"\"\"\n",
    "    f = featurize(text)\n",
    "    if f.length < 25: return 1\n",
    "    if f.exclam >= 3: return 1\n",
    "    if f.caps_ratio > 0.35: return 1\n",
    "    if f.unique_ratio < 0.4 and f.words > 6: return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def train_fake_review_model(texts: List[str], out_pkl: Path) -> Dict:\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    X, y = [], []\n",
    "    for t in texts:\n",
    "        f = featurize(t)\n",
    "        X.append([f.length, f.exclam, f.caps_ratio, f.unique_ratio, f.digits, f.words])\n",
    "        y.append(weak_label(t))\n",
    "    if not X:\n",
    "        X = [[10, 0, 0.0, 1.0, 0, 2]]\n",
    "        y = [0]\n",
    "\n",
    "    X = np.array(X, dtype=float)\n",
    "    y = np.array(y, dtype=int)\n",
    "\n",
    "    clf = LogisticRegression(max_iter=200)\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    ensure_dir(out_pkl.parent)\n",
    "    with open(out_pkl, \"wb\") as f:\n",
    "        pickle.dump(clf, f)\n",
    "\n",
    "    return {\n",
    "        \"trained\": True,\n",
    "        \"n_reviews\": int(len(texts)),\n",
    "        \"pos_rate\": float(y.mean()) if len(y) else 0.0,\n",
    "        \"features\": [\"length\", \"exclam\", \"caps_ratio\", \"unique_ratio\", \"digits\", \"words\"],\n",
    "    }\n",
    "\n",
    "\n",
    "# ------------------------------------ Main ------------------------------------\n",
    "def main():\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "\n",
    "    # Paths (raw strings) -> Path objects\n",
    "    csv_path = Path(DATA_CSV)\n",
    "    out_dir = ensure_dir(ARTIFACT_DIR)\n",
    "\n",
    "    # Pre-flight echo to prove paths are literal (avoids unicodeescape issues)\n",
    "    print(f\"[INFO] CSV path:      {csv_path}\")\n",
    "    print(f\"[INFO] Artifact dir:  {out_dir}\")\n",
    "\n",
    "    # 1) Save config snapshot (yaml)\n",
    "    yaml_out = out_dir / \"config.yaml\"\n",
    "    cfg = {\"data_csv\": str(csv_path), \"artifact_dir\": str(out_dir), **SETTINGS}\n",
    "    with open(yaml_out, \"w\", encoding=\"utf-8\") as f:\n",
    "        yaml.safe_dump(cfg, f, sort_keys=False, allow_unicode=True)\n",
    "\n",
    "    # 2) Load CSV & map columns\n",
    "    df = load_csv(csv_path)\n",
    "    cols = map_columns(df)\n",
    "    df = df.head(int(SETTINGS[\"top_n_products\"])).copy()\n",
    "\n",
    "    # 3) Build synthetic price series + pseudo review corpus\n",
    "    price_series_list: List[np.ndarray] = []\n",
    "    review_corpus: List[str] = []\n",
    "\n",
    "    title_col = cols[\"title\"]\n",
    "    desc_col = cols[\"description\"]\n",
    "    retail_col = cols[\"retail_price\"]\n",
    "    disc_col = cols[\"discounted_price\"]\n",
    "\n",
    "    for _, r in df.iterrows():\n",
    "        # base price\n",
    "        if disc_col and not pd.isna(r.get(disc_col)):\n",
    "            base = float(r.get(disc_col))\n",
    "        elif retail_col and not pd.isna(r.get(retail_col)):\n",
    "            base = float(r.get(retail_col))\n",
    "        else:\n",
    "            base = 100.0\n",
    "\n",
    "        series = synthetic_series(\n",
    "            base_price=base,\n",
    "            days=int(SETTINGS[\"seed_days\"]),\n",
    "            noise_frac=float(SETTINGS[\"price_noise_frac\"]),\n",
    "            deal_prob=float(SETTINGS[\"deal_drop_prob\"]),\n",
    "            deal_frac=float(SETTINGS[\"deal_drop_frac\"]),\n",
    "        )\n",
    "        price_series_list.append(series)\n",
    "\n",
    "        # pseudo reviews from description/title slices\n",
    "        text_src = str(r.get(desc_col)) if desc_col and not pd.isna(r.get(desc_col)) else str(r.get(title_col))\n",
    "        text_src = text_src or \"Good product with decent quality and value for money.\"\n",
    "        for chunk in [text_src[:140], text_src[140:280], text_src[280:420]]:\n",
    "            if chunk and len(chunk.strip()) > 10:\n",
    "                review_corpus.append(chunk.strip())\n",
    "\n",
    "    # 4) Train LSTM (or fallback) -> .h5\n",
    "    h5_out = out_dir / \"price_lstm.h5\"\n",
    "    lstm_metrics = train_lstm_and_save(\n",
    "        price_series_list,\n",
    "        seq_len=int(SETTINGS[\"lstm_sequence_len\"]),\n",
    "        epochs=int(SETTINGS[\"lstm_epochs\"]),\n",
    "        batch_size=int(SETTINGS[\"lstm_batch_size\"]),\n",
    "        h5_out=h5_out,\n",
    "    )\n",
    "\n",
    "    # 5) Train fake-review model -> .pkl\n",
    "    pkl_out = out_dir / \"fake_review_lr.pkl\"\n",
    "    fr_metrics = train_fake_review_model(review_corpus, pkl_out)\n",
    "\n",
    "    # 6) metrics.json\n",
    "    json_out = out_dir / \"metrics.json\"\n",
    "    metrics = {\n",
    "        \"price_model\": lstm_metrics,\n",
    "        \"fake_review_model\": fr_metrics,\n",
    "        \"counts\": {\"n_products\": int(len(price_series_list)), \"n_reviews\": int(len(review_corpus))},\n",
    "    }\n",
    "    with open(json_out, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metrics, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(\"\\n=== PriceSense ML Artifacts Ready ===\")\n",
    "    print(f\"H5   -> {h5_out}\")\n",
    "    print(f\"JSON -> {json_out}\")\n",
    "    print(f\"YAML -> {yaml_out}\")\n",
    "    print(f\"PKL  -> {pkl_out}\")\n",
    "    if not TF_AVAILABLE:\n",
    "        print(\"Note: TensorFlow not detected; wrote a safe placeholder .h5. Install TF for full LSTM training.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8fbd44-eb34-4127-add6-9181ed9cea92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (moviepy)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
