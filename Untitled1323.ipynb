{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e41c4349-f9b3-4e4e-896e-5fe84738a365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Fake Review Model – Test Metrics ===\n",
      "Accuracy : 0.9744\n",
      "Precision: 0.8750\n",
      "Recall   : 0.0228\n",
      "F1-score : 0.0444\n",
      "AUC      : 0.8948\n",
      "\n",
      "Saved plots:\n",
      " - Confusion matrix: C:\\Users\\sagni\\Downloads\\Price Sense\\confusion_matrix.png\n",
      " - ROC curve      : C:\\Users\\sagni\\Downloads\\Price Sense\\roc_curve.png\n",
      " - PR curve       : C:\\Users\\sagni\\Downloads\\Price Sense\\pr_curve.png\n",
      " - Accuracy curve : C:\\Users\\sagni\\Downloads\\Price Sense\\accuracy_vs_threshold.png\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    "    roc_auc_score,\n",
    "    precision_recall_curve,\n",
    "    average_precision_score,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ---------------------- PATHS (Windows-safe raw strings) ----------------------\n",
    "DATA_CSV = r\"C:\\Users\\sagni\\Downloads\\Price Sense\\archive\\flipkart_com-ecommerce_sample.csv\"\n",
    "ARTIFACT_DIR = r\"C:\\Users\\sagni\\Downloads\\Price Sense\"\n",
    "MODEL_PKL = Path(ARTIFACT_DIR) / \"fake_review_lr.pkl\"\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# --------------------- Column mapping / data helpers --------------------------\n",
    "def load_csv(csv_path: str | Path) -> pd.DataFrame:\n",
    "    p = Path(csv_path)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"CSV not found: {p}\")\n",
    "    try:\n",
    "        return pd.read_csv(p, encoding=\"utf-8\")\n",
    "    except UnicodeDecodeError:\n",
    "        return pd.read_csv(p, encoding=\"latin1\")\n",
    "\n",
    "\n",
    "def map_columns(df: pd.DataFrame) -> Dict[str, str | None]:\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "\n",
    "    def find(name: str):\n",
    "        for k, v in cols.items():\n",
    "            if name in k:\n",
    "                return v\n",
    "        return None\n",
    "\n",
    "    return {\n",
    "        \"title\": find(\"product_name\") or find(\"title\") or list(df.columns)[0],\n",
    "        \"description\": find(\"description\") or find(\"product_description\"),\n",
    "    }\n",
    "\n",
    "\n",
    "# -------------------- Weak labeling & featurization (same as train) ----------\n",
    "@dataclass\n",
    "class ReviewFeatures:\n",
    "    length: int\n",
    "    exclam: int\n",
    "    caps_ratio: float\n",
    "    unique_ratio: float\n",
    "    digits: int\n",
    "    words: int\n",
    "\n",
    "\n",
    "def featurize(text: str) -> ReviewFeatures:\n",
    "    import re\n",
    "    t = text or \"\"\n",
    "    words = re.findall(r\"[A-Za-z0-9']+\", t)\n",
    "    words_lower = [w.lower() for w in words]\n",
    "    unique_ratio = (len(set(words_lower)) / (len(words_lower) + 1e-6))\n",
    "    caps_ratio = (sum(1 for c in t if c.isupper()) / (len(t) + 1e-6))\n",
    "    digits = sum(ch.isdigit() for ch in t)\n",
    "    return ReviewFeatures(\n",
    "        length=len(t),\n",
    "        exclam=t.count(\"!\"),\n",
    "        caps_ratio=float(caps_ratio),\n",
    "        unique_ratio=float(unique_ratio),\n",
    "        digits=int(digits),\n",
    "        words=len(words),\n",
    "    )\n",
    "\n",
    "\n",
    "def weak_label(text: str) -> int:\n",
    "    \"\"\"Heuristic: 1 = suspicious (fake-ish), 0 = genuine.\"\"\"\n",
    "    f = featurize(text)\n",
    "    if f.length < 25: return 1\n",
    "    if f.exclam >= 3: return 1\n",
    "    if f.caps_ratio > 0.35: return 1\n",
    "    if f.unique_ratio < 0.4 and f.words > 6: return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def vectorize(f: ReviewFeatures) -> List[float]:\n",
    "    return [f.length, f.exclam, f.caps_ratio, f.unique_ratio, f.digits, f.words]\n",
    "\n",
    "\n",
    "# ------------------------------ Plot helpers ---------------------------------\n",
    "def save_confusion_heatmap(y_true, y_pred, out_path: Path, normalize: bool = True):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    if normalize:\n",
    "        cm = cm.astype(float) / cm.sum(axis=1, keepdims=True).clip(min=1.0)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(5, 4), dpi=140)\n",
    "    im = ax.imshow(cm, interpolation=\"nearest\", cmap=\"Blues\")\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    classes = [\"Genuine (0)\", \"Suspicious (1)\"]\n",
    "    ax.set(\n",
    "        xticks=np.arange(len(classes)),\n",
    "        yticks=np.arange(len(classes)),\n",
    "        xticklabels=classes,\n",
    "        yticklabels=classes,\n",
    "        ylabel=\"True label\",\n",
    "        xlabel=\"Predicted label\",\n",
    "        title=\"Confusion Matrix\" + (\" (Normalized)\" if normalize else \"\"),\n",
    "    )\n",
    "\n",
    "    # Labels on cells\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(\n",
    "                j, i,\n",
    "                f\"{cm[i, j]:.2f}\" if normalize else f\"{cm[i, j]:d}\",\n",
    "                ha=\"center\",\n",
    "                va=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "                fontsize=10,\n",
    "            )\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(out_path, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def save_roc_curve(y_true, y_score, out_path: Path):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    auc = roc_auc_score(y_true, y_score)\n",
    "    fig, ax = plt.subplots(figsize=(5, 4), dpi=140)\n",
    "    ax.plot(fpr, tpr, label=f\"AUC = {auc:.3f}\")\n",
    "    ax.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "    ax.set_xlabel(\"False Positive Rate\")\n",
    "    ax.set_ylabel(\"True Positive Rate\")\n",
    "    ax.set_title(\"ROC Curve\")\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(out_path, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def save_pr_curve(y_true, y_score, out_path: Path):\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_score)\n",
    "    ap = average_precision_score(y_true, y_score)\n",
    "    fig, ax = plt.subplots(figsize=(5, 4), dpi=140)\n",
    "    ax.plot(recall, precision, label=f\"AP = {ap:.3f}\")\n",
    "    ax.set_xlabel(\"Recall\")\n",
    "    ax.set_ylabel(\"Precision\")\n",
    "    ax.set_title(\"Precision–Recall Curve\")\n",
    "    ax.legend(loc=\"lower left\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(out_path, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def save_accuracy_threshold_curve(y_true, y_score, out_path: Path):\n",
    "    thresholds = np.linspace(0.05, 0.95, 19)\n",
    "    accs, precs, recs, f1s = [], [], [], []\n",
    "    for t in thresholds:\n",
    "        y_pred = (y_score >= t).astype(int)\n",
    "        accs.append(accuracy_score(y_true, y_pred))\n",
    "        precs.append(precision_score(y_true, y_pred, zero_division=0))\n",
    "        recs.append(recall_score(y_true, y_pred, zero_division=0))\n",
    "        f1s.append(f1_score(y_true, y_pred, zero_division=0))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 4), dpi=140)\n",
    "    ax.plot(thresholds, accs, label=\"Accuracy\")\n",
    "    ax.plot(thresholds, precs, label=\"Precision\")\n",
    "    ax.plot(thresholds, recs, label=\"Recall\")\n",
    "    ax.plot(thresholds, f1s, label=\"F1\")\n",
    "    ax.set_xlabel(\"Probability Threshold\")\n",
    "    ax.set_ylabel(\"Score\")\n",
    "    ax.set_title(\"Scores vs Threshold\")\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(out_path, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "# ------------------------------------ Main -----------------------------------\n",
    "def main():\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "\n",
    "    out_dir = Path(ARTIFACT_DIR)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 1) Load data and build pseudo-reviews (same logic as training)\n",
    "    df = load_csv(DATA_CSV)\n",
    "    cols = map_columns(df)\n",
    "    title_col = cols[\"title\"]\n",
    "    desc_col = cols[\"description\"]\n",
    "\n",
    "    texts: List[str] = []\n",
    "    # each product -> up to 3 snippets (for a decent sample size)\n",
    "    for _, r in df.iterrows():\n",
    "        text_src = str(r.get(desc_col)) if desc_col and not pd.isna(r.get(desc_col)) else str(r.get(title_col))\n",
    "        text_src = text_src or \"\"\n",
    "        chunks = [text_src[:140], text_src[140:280], text_src[280:420]]\n",
    "        for ch in chunks:\n",
    "            if ch and len(ch.strip()) > 10:\n",
    "                texts.append(ch.strip())\n",
    "\n",
    "    if not texts:\n",
    "        raise RuntimeError(\"No review-like text could be generated from the CSV to evaluate on.\")\n",
    "\n",
    "    # 2) Features & labels\n",
    "    X = np.array([vectorize(featurize(t)) for t in texts], dtype=float)\n",
    "    y = np.array([weak_label(t) for t in texts], dtype=int)\n",
    "\n",
    "    # 3) Load model\n",
    "    if not MODEL_PKL.exists():\n",
    "        raise FileNotFoundError(f\"Model file not found: {MODEL_PKL}\\nRun your training script first.\")\n",
    "    with open(MODEL_PKL, \"rb\") as f:\n",
    "        clf = pickle.load(f)\n",
    "\n",
    "    # 4) Split (evaluate on a held-out test set)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.25, random_state=42, stratify=y if len(np.unique(y)) > 1 else None\n",
    "    )\n",
    "\n",
    "    # Note: we do NOT re-train the model here; we evaluate the already-trained one.\n",
    "    # If you prefer CV or re-fitting, you could fit clf on X_train,y_train first.\n",
    "\n",
    "    # 5) Predict scores & class labels on test set\n",
    "    if hasattr(clf, \"predict_proba\"):\n",
    "        y_score = clf.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        # fallback: decision_function if available\n",
    "        if hasattr(clf, \"decision_function\"):\n",
    "            z = clf.decision_function(X_test)\n",
    "            # scale to 0..1 via logistic-ish transform for curves\n",
    "            y_score = 1.0 / (1.0 + np.exp(-z))\n",
    "        else:\n",
    "            # worst case: use predictions as {0,1} float\n",
    "            y_score = clf.predict(X_test).astype(float)\n",
    "\n",
    "    y_pred = (y_score >= 0.5).astype(int)\n",
    "\n",
    "    # 6) Metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "    try:\n",
    "        auc = roc_auc_score(y_test, y_score)\n",
    "    except Exception:\n",
    "        auc = float(\"nan\")\n",
    "\n",
    "    print(\"=== Fake Review Model – Test Metrics ===\")\n",
    "    print(f\"Accuracy : {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall   : {rec:.4f}\")\n",
    "    print(f\"F1-score : {f1:.4f}\")\n",
    "    print(f\"AUC      : {auc:.4f}\")\n",
    "\n",
    "    # 7) Plots\n",
    "    save_confusion_heatmap(y_test, y_pred, out_dir / \"confusion_matrix.png\", normalize=True)\n",
    "    save_roc_curve(y_test, y_score, out_dir / \"roc_curve.png\")\n",
    "    save_pr_curve(y_test, y_score, out_dir / \"pr_curve.png\")\n",
    "    save_accuracy_threshold_curve(y_test, y_score, out_dir / \"accuracy_vs_threshold.png\")\n",
    "\n",
    "    print(\"\\nSaved plots:\")\n",
    "    print(f\" - Confusion matrix: {out_dir / 'confusion_matrix.png'}\")\n",
    "    print(f\" - ROC curve      : {out_dir / 'roc_curve.png'}\")\n",
    "    print(f\" - PR curve       : {out_dir / 'pr_curve.png'}\")\n",
    "    print(f\" - Accuracy curve : {out_dir / 'accuracy_vs_threshold.png'}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df054f58-34b7-4d58-bb06-88681238398f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (moviepy)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
